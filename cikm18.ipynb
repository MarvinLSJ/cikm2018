{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liushijing/anaconda3/envs/python27/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "# reload(sys) \n",
    "# sys.setdefaultencoding('utf-8')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from featwheel.utils import NgramUtil,DistanceUtil, LogUtil, MathUtil\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from models import NLINet\n",
    "from mutils import get_optimizer\n",
    "from data import get_nli, get_batch\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer1 = SnowballStemmer('spanish')\n",
    "snowball_stemmer2 = SnowballStemmer('english')\n",
    "snowball_stemmer1.stem\n",
    "snowball_stemmer2.stem\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops1 = set(stopwords.words(\"spanish\"))\n",
    "stops2 = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_en_sp = pd.read_csv('./input/cikm_english_train_20180516.txt',sep='\t', header=None,error_bad_lines=False)\n",
    "df_train_sp_en = pd.read_csv('./input/cikm_spanish_train_20180516.txt',sep='\t', header=None,error_bad_lines=False)\n",
    "df_train_en_sp.columns = ['english1', 'spanish1', 'english2', 'spanish2', 'result']\n",
    "df_train_sp_en.columns = ['spanish1', 'english1', 'english2', 'spanish2', 'result']\n",
    "\n",
    "# Evaluation data\n",
    "test_data = pd.read_csv('./input/cikm_test_a_20180516.txt', sep='\t', header=None,error_bad_lines=False)\n",
    "test_data.columns = ['spanish1', 'spanish2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spanish1</th>\n",
       "      <th>spanish2</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hola, hago clic en el producto recibido</td>\n",
       "      <td>Compré un producto y no he recibido un correo ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¡Hola! Cerré la disputa el 21 de mayo de 2017 ...</td>\n",
       "      <td>No obtuve el reembolso de mi dinero. Han pasad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ordené de España a España ahora que mandan el ...</td>\n",
       "      <td>Mi pedido llegó pero el color es diferente al ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Debo pagar impuestos personalizados?</td>\n",
       "      <td>Cómo pagar los derechos de aduana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No recibí mi pedido?</td>\n",
       "      <td>Mi pedido muestra que no he pagado, pero lo hice</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            spanish1  \\\n",
       "0            hola, hago clic en el producto recibido   \n",
       "1  ¡Hola! Cerré la disputa el 21 de mayo de 2017 ...   \n",
       "2  Ordené de España a España ahora que mandan el ...   \n",
       "3              ¿Debo pagar impuestos personalizados?   \n",
       "4                               No recibí mi pedido?   \n",
       "\n",
       "                                            spanish2  result  \n",
       "0  Compré un producto y no he recibido un correo ...       0  \n",
       "1  No obtuve el reembolso de mi dinero. Han pasad...       0  \n",
       "2  Mi pedido llegó pero el color es diferente al ...       0  \n",
       "3                  Cómo pagar los derechos de aduana       1  \n",
       "4   Mi pedido muestra que no he pagado, pero lo hice       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1 = pd.DataFrame(pd.concat([df_train_en_sp['spanish1'],df_train_sp_en['spanish1']], axis=0))\n",
    "train2 = pd.DataFrame(pd.concat([df_train_en_sp['spanish2'],df_train_sp_en['spanish2']], axis=0))\n",
    "train = pd.concat([train1,train2],axis=1).reset_index()\n",
    "train = train.drop(['index'],axis=1)\n",
    "result = pd.DataFrame(pd.concat([df_train_en_sp['result'],df_train_sp_en['result']], axis=0)).reset_index()\n",
    "result = result.drop(['index'],axis=1)\n",
    "# pd.get_dummies(result['result']).head()\n",
    "train['result'] = result\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subchar(text):\n",
    "\ttext=text.replace(\"á\", \"a\")\n",
    "\ttext=text.replace(\"ó\", \"o\")\n",
    "\ttext=text.replace(\"é\", \"e\")\t\n",
    "\ttext=text.replace(\"í\", \"i\")\n",
    "\ttext=text.replace(\"ú\", \"u\")\t\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#西班牙语缩写还原#\n",
    "def stemSpanish(df):\n",
    "\tdf['spanish1'] = df.spanish1.map(lambda x: ' '.join([snowball_stemmer1.stem(word) for word in \n",
    "                                                         nltk.word_tokenize(x.lower().decode('utf-8'))]).encode('utf-8'))\n",
    "\tdf['spanish2'] = df.spanish2.map(lambda x: ' '.join([snowball_stemmer1.stem(word) for word in \n",
    "                                                         nltk.word_tokenize(x.lower().decode('utf-8'))]).encode('utf-8'))\n",
    "def stemEnglish(df):\n",
    "    df['english1'] =  df.english1.map(lambda x:' '.join([snowball_stemmer2.stem(word) for word in \n",
    "                                                        nltk.word_tokenize(x.lower().decode('utf-8'))]).encode('utf-8'))\n",
    "    df['english2'] =  df.english2.map(lambda x:' '.join([snowball_stemmer2.stem(word) for word in \n",
    "                                                        nltk.word_tokenize(x.lower().decode('utf-8'))]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpanishStopWords(df, stop):\n",
    "\tdf['spanish1'] = df.spanish1.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x.decode('utf-8'))\n",
    "                                                         if word not in stops1]).encode('utf-8'))\n",
    "\tdf['spanish2'] = df.spanish2.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x.decode('utf-8'))\n",
    "                                                         if word not in stops1]).encode('utf-8'))\n",
    "    \n",
    "def removeEnglishStopWords(df, stop):\n",
    "\tdf['english1'] = df.english1.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x.decode('utf-8'))\n",
    "                                                         if word not in stops2]).encode('utf-8'))\n",
    "\tdf['english2'] = df.english2.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x.decode('utf-8'))\n",
    "                                                         if word not in stops2]).encode('utf-8'))\n",
    "    \n",
    "def removeEnglishSigns(df):\n",
    "    df['english1'] = df.english1.map(lambda x: re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]','',x))\n",
    "    df['english2'] = df.english2.map(lambda x: re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]','',x))\n",
    "def removeSpanishSigns(df):\n",
    "    df['spanish1'] = df.spanish1.map(lambda x: re.sub(r'[_\"\\-;%()|+&=*%.,¡!¿?:#$@\\[\\]/]','',x))\n",
    "    df['spanish2'] = df.spanish2.map(lambda x: re.sub(r'[_\"\\-;%()|+&=*%.,¡!¿?:#$@\\[\\]/]','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spanish1</th>\n",
       "      <th>spanish2</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hola, hago clic en el producto recibido</td>\n",
       "      <td>Compré un producto y no he recibido un correo ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¡Hola! Cerré la disputa el 21 de mayo de 2017 ...</td>\n",
       "      <td>No obtuve el reembolso de mi dinero. Han pasad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ordené de España a España ahora que mandan el ...</td>\n",
       "      <td>Mi pedido llegó pero el color es diferente al ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Debo pagar impuestos personalizados?</td>\n",
       "      <td>Cómo pagar los derechos de aduana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No recibí mi pedido?</td>\n",
       "      <td>Mi pedido muestra que no he pagado, pero lo hice</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            spanish1  \\\n",
       "0            hola, hago clic en el producto recibido   \n",
       "1  ¡Hola! Cerré la disputa el 21 de mayo de 2017 ...   \n",
       "2  Ordené de España a España ahora que mandan el ...   \n",
       "3              ¿Debo pagar impuestos personalizados?   \n",
       "4                               No recibí mi pedido?   \n",
       "\n",
       "                                            spanish2  result  \n",
       "0  Compré un producto y no he recibido un correo ...       0  \n",
       "1  No obtuve el reembolso de mi dinero. Han pasad...       0  \n",
       "2  Mi pedido llegó pero el color es diferente al ...       0  \n",
       "3                  Cómo pagar los derechos de aduana       1  \n",
       "4   Mi pedido muestra que no he pagado, pero lo hice       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 Spanish\n",
    "stemSpanish(train)\n",
    "removeSpanishSigns(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spanish1</th>\n",
       "      <th>spanish2</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hol  hag clic en el product recib</td>\n",
       "      <td>compr un product y no he recib un corre electr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hol  cerr la disput el 21 de may de 2017 y dic...</td>\n",
       "      <td>no obtuv el reembols de mi diner  han pas dos ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>orden de españ a españ ahor que mand el ped de...</td>\n",
       "      <td>mi ped lleg per el color es diferent al que ped</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deb pag impuest personaliz</td>\n",
       "      <td>com pag los derech de aduan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no recib mi ped</td>\n",
       "      <td>mi ped muestr que no he pag  per lo hic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            spanish1  \\\n",
       "0                  hol  hag clic en el product recib   \n",
       "1  hol  cerr la disput el 21 de may de 2017 y dic...   \n",
       "2  orden de españ a españ ahor que mand el ped de...   \n",
       "3                        deb pag impuest personaliz    \n",
       "4                                   no recib mi ped    \n",
       "\n",
       "                                            spanish2  result  \n",
       "0  compr un product y no he recib un corre electr...       0  \n",
       "1  no obtuv el reembols de mi diner  han pas dos ...       0  \n",
       "2   mi ped lleg per el color es diferent al que ped        0  \n",
       "3                        com pag los derech de aduan       1  \n",
       "4            mi ped muestr que no he pag  per lo hic       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemSpanish(test_data)\n",
    "removeSpanishSigns(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train, Dev, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train\n",
    "train_data.columns = ['s1','s2','label']\n",
    "test_data.columns = ['s1','s2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按比例分割 train dev test\n",
    "total_len = train_data.shape[0]\n",
    "train_len = int(total_len * 0.8)\n",
    "dev_len = train_len + int(total_len * 0.15)\n",
    "test_len = dev_len + int(total_len * 0.05)\n",
    "\n",
    "train = train_data.loc[0:train_len-1]\n",
    "valid = train_data.loc[train_len:dev_len-1]\n",
    "test = train_data.loc[dev_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W2V_PATH = \"input/wiki.es.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vec = build_vocab(train['s1'].tolist() + train['s2'].tolist() +\n",
    "#                        valid['s1'].tolist() + valid['s2'].tolist() +\n",
    "#                        test['s1'].tolist() + test['s2'].tolist(), W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split in ['s1', 's2']:\n",
    "#     for data_type in ['train', 'valid', 'test']:\n",
    "#         eval(data_type)[split] = np.array([['<s>'] +\n",
    "#             [word for word in sent.split() if word in word_vec] +\n",
    "#             ['</s>'] for sent in eval(data_type)[split]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# es_vec = KeyedVectors.load_word2vec_format('input/wiki.es.vec') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent.split():\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    word_dict['<unk>'] = ''\n",
    "    return word_dict\n",
    "all_sent = train_data['s1'].tolist() + train_data['s2'].tolist() + test_data['s1'].tolist() + test_data['s2'].tolist()\n",
    "word_dict = get_word_dict(all_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word_dict, embedding_path):\n",
    "    # create word_vec with embedding vectors\n",
    "    word_vec = {}\n",
    "    with open(embedding_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}/{1} words with embedding vectors'.format(\n",
    "                len(word_vec), len(word_dict)))\n",
    "    missing_word_num = len(word_dict) - len(word_vec)\n",
    "    missing_ratio = round(float(missing_word_num)/len(word_dict),4)*100\n",
    "    print('Missing Ratio: {}%'.format(missing_ratio))\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2403/3894 words with embedding vectors\n",
      "Missing Ratio: 38.29%\n"
     ]
    }
   ],
   "source": [
    "embedding_path = 'input/wiki.es.vec'\n",
    "word_vec = get_embedding(word_dict, embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {}\n",
    "value = 0\n",
    "for word in word_dict:\n",
    "    vocab_to_int[word] = value\n",
    "    value += 1\n",
    "        \n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling unknown words' embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled Unk words' embeddings\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word not in word_vec:\n",
    "        # If word not in word_vec, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        word_vec[word] = new_embedding\n",
    "print \"Filled Unk words' embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need to use 300 for embedding dimensions to match given vectors.\n",
    "# embedding_dim = 300\n",
    "# nb_words = len(vocab_to_int)\n",
    "\n",
    "# # Create matrix with default values of zero\n",
    "# word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "# for word, i in vocab_to_int.items():\n",
    "#     if word in word_vec:\n",
    "#         word_embedding_matrix[i] = word_vec[word]\n",
    "#     else:\n",
    "#         # If word not in word_vec, create a random embedding for it\n",
    "#         new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "#         word_vec[word] = new_embedding\n",
    "#         word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# # Check if value matches len(vocab_to_int)\n",
    "# print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f18b3ca1b123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         eval(data_type)[split] = np.array([['<s>'] +\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             ['</s>'] for sent in eval(data_type)[split].tolist()])\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "for split in ['s1', 's2']:\n",
    "    for data_type in ['train', 'valid', 'test']:\n",
    "        eval(data_type)[split] = np.array([['<s>'] +\n",
    "            [word for word in sent.split() if word in word_vec] +\n",
    "            ['</s>'] for sent in eval(data_type)[split].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_ints(text):\n",
    "#     '''Convert words in text to an integer.\n",
    "#        If word is not in vocab_to_int, use UNK's integer.\n",
    "#        Total the number of words and UNKs.\n",
    "#        Add EOS token to the end of texts'''\n",
    "#     ints = []\n",
    "#     unk_count = 0\n",
    "#     for sentence in text:\n",
    "#         sentence_ints = [vocab_to_int['<s>']]\n",
    "#         for word in sentence.split():\n",
    "#             if word in vocab_to_int:\n",
    "#                 sentence_ints.append(vocab_to_int[word])\n",
    "#             else:\n",
    "#                 sentence_ints.append(vocab_to_int['<unk>'])\n",
    "#                 unk_count += 1\n",
    "#             sentence_ints.append(vocab_to_int['</s>'])\n",
    "#         ints.append(sentence_ints)\n",
    "#     return ints, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize_sent,unk_count = convert_to_ints(all_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='InferSent training')\n",
    "# paths\n",
    "parser.add_argument(\"--outputdir\", type=str, default='savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='model.pickle')\n",
    "\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=20)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0., help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=0, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"sgd,lr=0.1\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='InferSent', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=512, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=3, help=\"entailment/neutral/contradiction\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "params, _ = parser.parse_known_args()\n",
    "params.word_emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, decay=0.99, dpout_fc=0.0, dpout_model=0.0, enc_lstm_dim=2048, encoder_type='InferSent', fc_dim=512, lrshrink=5, max_norm=5.0, minlr=1e-05, n_classes=3, n_enc_layers=1, n_epochs=20, nonlinear_fc=0, optimizer='sgd,lr=0.1', outputdir='savedir/', outputmodelname='model.pickle', pool_type='max', word_emb_dim=300)\n"
     ]
    }
   ],
   "source": [
    "print params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLINet(\n",
      "  (encoder): InferSent(\n",
      "    (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (2): Linear(in_features=512, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model config\n",
    "config_nli_model = {\n",
    "    'n_words'        :  len(word_vec)          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "\n",
    "# model\n",
    "encoder_types = ['InferSent', 'BLSTMprojEncoder', 'BGRUlastEncoder',\n",
    "                 'InnerAttentionMILAEncoder', 'InnerAttentionYANGEncoder',\n",
    "                 'InnerAttentionNAACLEncoder', 'ConvNetEncoder', 'LSTMEncoder']\n",
    "assert params.encoder_type in encoder_types, \"encoder_type must be in \" + \\\n",
    "                                             str(encoder_types)\n",
    "nli_net = NLINet(config_nli_model)\n",
    "print(nli_net)\n",
    "\n",
    "# loss\n",
    "weight = torch.FloatTensor(params.n_classes).fill_(1)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn.size_average = False\n",
    "\n",
    "# optimizer\n",
    "optim_fn, optim_params = get_optimizer(params.optimizer)\n",
    "optimizer = optim_fn(nli_net.parameters(), **optim_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRAIN\n",
    "\"\"\"\n",
    "val_acc_best = -1e10\n",
    "adam_stop = False\n",
    "stop_training = False\n",
    "lr = optim_params['lr'] if 'sgd' in params.optimizer else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainepoch(epoch):\n",
    "    print('\\nTRAINING : Epoch ' + str(epoch))\n",
    "    nli_net.train()\n",
    "    all_costs = []\n",
    "    logs = []\n",
    "    words_count = 0\n",
    "\n",
    "    last_time = time.time()\n",
    "    correct = 0.\n",
    "    # shuffle the data\n",
    "    permutation = np.random.permutation(len(train['s1']))\n",
    "\n",
    "    s1 = train['s1'][permutation]\n",
    "    s2 = train['s2'][permutation]\n",
    "    target = train['label'][permutation]\n",
    "\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * params.decay if epoch>1\\\n",
    "        and 'sgd' in params.optimizer else optimizer.param_groups[0]['lr']\n",
    "    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for stidx in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size].tolist(),\n",
    "                                     word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size].tolist(),\n",
    "                                     word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch), Variable(s2_batch)\n",
    "        tgt_batch = Variable(torch.LongTensor(target[stidx:stidx + params.batch_size]))\n",
    "        k = s1_batch.size(1)  # actual batch size\n",
    "\n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        assert len(pred) == len(s1[stidx:stidx + params.batch_size])\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn(output, tgt_batch)\n",
    "        all_costs.append(loss.data[0])\n",
    "        words_count += (s1_batch.nelement() + s2_batch.nelement()) / params.word_emb_dim\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping (off by default)\n",
    "        shrink_factor = 1\n",
    "        total_norm = 0\n",
    "\n",
    "        for p in nli_net.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.grad.data.div_(k)  # divide by the actual batch size\n",
    "                total_norm += p.grad.data.norm() ** 2\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "\n",
    "        if total_norm > params.max_norm:\n",
    "            shrink_factor = params.max_norm / total_norm\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # current lr (no external \"lr\", for adam)\n",
    "        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update\n",
    "\n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "\n",
    "        if len(all_costs) == 100:\n",
    "            logs.append('{0} ; loss {1} ; sentence/s {2} ; words/s {3} ; accuracy train : {4}'.format(\n",
    "                            stidx, round(np.mean(all_costs), 2),\n",
    "                            int(len(all_costs) * params.batch_size / (time.time() - last_time)),\n",
    "                            int(words_count * 1.0 / (time.time() - last_time)),\n",
    "                            round(100.*correct/(stidx+k), 2)))\n",
    "            print(logs[-1])\n",
    "            last_time = time.time()\n",
    "            words_count = 0\n",
    "            all_costs = []\n",
    "    train_acc = round(100 * correct/len(s1), 2)\n",
    "    print('results : epoch {0} ; mean accuracy train : {1}'\n",
    "          .format(epoch, train_acc))\n",
    "    return train_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-d0257f733e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 's1' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING : Epoch 1\n",
      "Learning rate : 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liushijing/anaconda3/envs/python27/lib/python2.7/site-packages/ipykernel_launcher.py:40: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-b760b4103375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_training\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0meval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-ba687c915f35>\u001b[0m in \u001b[0;36mtrainepoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# gradient clipping (off by default)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liushijing/anaconda3/envs/python27/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liushijing/anaconda3/envs/python27/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "\n",
    "while not stop_training and epoch <= params.n_epochs:\n",
    "    train_acc = trainepoch(epoch)\n",
    "    eval_acc = evaluate(epoch, 'valid')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch,eval_type='valid', final_eval=False):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "\n",
    "    if eval_type == 'valid':\n",
    "        print('\\nVALIDATION : Epoch {0}'.format(epoch))\n",
    "\n",
    "    s1 = valid['s1'] if eval_type == 'valid' else test['s1']\n",
    "    s2 = valid['s2'] if eval_type == 'valid' else test['s2']\n",
    "    target = valid['label'] if eval_type == 'valid' else test['label']\n",
    "\n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size].tolist(), word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size].tolist(), word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch), Variable(s2_batch)\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size]))\n",
    "\n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "\n",
    "    # save model\n",
    "    eval_acc = round(100 * correct / len(s1), 2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : epoch {0} ; mean accuracy {1} :\\\n",
    "              {2}'.format(epoch, eval_type, eval_acc))\n",
    "\n",
    "    if eval_type == 'valid' and epoch <= params.n_epochs:\n",
    "        if eval_acc > val_acc_best:\n",
    "            print('saving model at epoch {0}'.format(epoch))\n",
    "            if not os.path.exists(params.outputdir):\n",
    "                os.makedirs(params.outputdir)\n",
    "            torch.save(nli_net.state_dict(), os.path.join(params.outputdir,\n",
    "                       params.outputmodelname))\n",
    "            val_acc_best = eval_acc\n",
    "        else:\n",
    "            if 'sgd' in params.optimizer:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / params.lrshrink\n",
    "                print('Shrinking lr by : {0}. New lr = {1}'\n",
    "                      .format(params.lrshrink,\n",
    "                              optimizer.param_groups[0]['lr']))\n",
    "                if optimizer.param_groups[0]['lr'] < params.minlr:\n",
    "                    stop_training = True\n",
    "            if 'adam' in params.optimizer:\n",
    "                # early stopping (at 2nd decrease in accuracy)\n",
    "                stop_training = adam_stop\n",
    "                adam_stop = True\n",
    "    return eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
